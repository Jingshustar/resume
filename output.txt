File: notion_manager.py
==================================================
import os
from notion_client import Client
import pandas as pd
from dotenv import load_dotenv
import requests

# Load environment variables
load_dotenv()

# Get Notion API key from environment variables
NOTION_API_KEY = os.getenv("NOTION_API_KEY")

notion_schema = {
    "job_position_title": {
        "type": "title",
        "notion_prop_name": "Job Role"
    },
    "job_id": {
        "type": "number",
        "notion_prop_name": "Job ID"
    },
    "job_position_link": {
        "type": "url",
        "notion_prop_name": "Job Link"
    },
    "company_name": {
        "type": "select",
        "notion_prop_name": "Company"
    },
    "location": {
        "type": "select",
        "notion_prop_name": "Location"
    },
    "days_ago": {
        "type": "rich_text",
        "notion_prop_name": "Posted"
    },
    "no_of_applicants": {
        "type": "number",
        "notion_prop_name": "Applicants"
    },
    "salary": {
        "type": "rich_text",
        "notion_prop_name": "Salary"
    },
    "workplace": {
        "type": "select",
        "notion_prop_name": "Workplace"
    },
    "job_type": {
        "type": "select",
        "notion_prop_name": "Job Type"
    },
    "experience_level": {
        "type": "select",
        "notion_prop_name": "Experience Level"
    },
    "industry": {
        "type": "select",
        "notion_prop_name": "Industry"
    },
    "is_easy_apply": {
        "type": "checkbox",
        "notion_prop_name": "Easy Apply"
    },
    "apply_link": {
        "type": "url",
        "notion_prop_name": "Apply Link"
    },
    "posted_date": {
        "type": "date",
        "notion_prop_name": "Posted Date"
    },
    "top_skills": {
        "type": "multi_select",
        "notion_prop_name": "Top Skills"
    },
    "job_category": {
        "type": "select",
        "notion_prop_name": "Job Category"
    }
}

class NotionManager:
    def __init__(self, database_id):
        self.notion = Client(auth=NOTION_API_KEY)
        self.database_id = database_id

    def create_property(self, property_name, property_type):
        """Create a new property in the Notion database"""
        try:
            self.notion.databases.update(
                database_id=self.database_id,
                properties={
                    property_name: {
                        "type": property_type,
                        property_type: {}
                    }
                }
            )
            print(f"Property '{property_name}' of type '{property_type}' created successfully.")
        except Exception as e:
            print(f"Error creating property: {e}")

    def sync_to_notion(self, df):
        """Sync DataFrame to Notion database"""
        for _, row in df.iterrows():
            properties = {}
            for col, prop_data in notion_schema.items():
                notion_prop_name = prop_data["notion_prop_name"]
                notion_type = prop_data["type"]
                value = row[col]

                if notion_type == "title":
                    properties[notion_prop_name] = {"title": [{"text": {"content": str(value)}}]}
                elif notion_type == "rich_text":
                    properties[notion_prop_name] = {"rich_text": [{"text": {"content": str(value)}}]}
                elif notion_type == "number":
                    properties[notion_prop_name] = {"number": float(value) if pd.notna(value) else None}
                elif notion_type == "select":
                    properties[notion_prop_name] = {"select": {"name": str(value).replace(",", "-")}}
                elif notion_type == "multi_select":
                    properties[notion_prop_name] = {"multi_select": [{"name": item.strip()} for item in str(value).split(',')]}
                elif notion_type == "date":
                    properties[notion_prop_name] = {"date": {"start": str(value), "time_zone": "America/Montreal"}}
                elif notion_type == "checkbox":
                    properties[notion_prop_name] = {"checkbox": bool(value)}
                elif notion_type == "url":
                    properties[notion_prop_name] = {"url": str(value)}

            try:
                page = self.notion.pages.create(
                    parent={"database_id": self.database_id},
                    properties=properties,
                    icon={"type": "external", "external": {"url": row['company_logo']}}
                )
                self.add_detailed_content(page["id"], row)
                print(f"Row added successfully: {row['job_id']}")
            except Exception as e:
                print(f"Error adding row: {row['job_id']}. Error: {e}")

    def add_detailed_content(self, page_id, row):
        """Add job_description, why_me, and why_this_company as blocks inside the page"""
        
        def create_paragraph_blocks(content):
            blocks = []
            while len(content) > 0:
                block_content = content[:2000]
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": block_content}}]
                    }
                })
                content = content[2000:]
            return blocks

        blocks = [
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "Job Description"}}]
                }
            },
        ]
        blocks.extend(create_paragraph_blocks(row['job_description']))

        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "Why This Company"}}]
            }
        })
        blocks.extend(create_paragraph_blocks(row['why_this_company']))

        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "Why Me"}}]
            }
        })
        blocks.extend(create_paragraph_blocks(row['why_me']))

        self.notion.blocks.children.append(page_id, children=blocks)

    def one_way_sync(self, df):
        """Perform one-way sync from DataFrame to Notion"""
        # Create properties if they don't exist
        # for prop_name, prop_data in notion_schema.items():
        #     self.create_property(prop_data["notion_prop_name"], prop_data["type"])

        # Sync data to Notion
        self.sync_to_notion(df)

# def main():
#     # Initialize NotionManager with your database ID
#     database_id = "7585377689d14a70bce0e38935403a1b"
#     notion_manager = NotionManager(database_id)

#     # # Read CSV and convert to DataFrame
#     df = pd.read_csv("job_data.csv")

#     # # Perform one-way sync
#     notion_manager.one_way_sync(df)

# if __name__ == "__main__":
#     main()


==================================================

File: datamanger.py
==================================================
import mysql.connector
from mysql.connector import Error
import os
import sys

# current_dir = os.path.dirname(os.path.abspath(__file__))
# sys.path.insert(0, current_dir)

class DataManager:
    def __init__(self, host, user, password, database):
        self.host = host
        self.user = user
        self.password = password
        self.database = database
        self.connection = None
        self.cursor = None

    def connect(self):
        try:
            # First, connect to MySQL server without specifying a database
            self.connection = mysql.connector.connect(
                host=self.host,
                user=self.user,
                password=self.password
            )
            
            if self.connection.is_connected():
                self.cursor = self.connection.cursor()
                
                # Check if the database exists
                self.cursor.execute(f"SHOW DATABASES LIKE '{self.database}'")
                result = self.cursor.fetchone()
                
                if not result:
                    # Create the database if it doesn't exist
                    self.cursor.execute(f"CREATE DATABASE {self.database}")
                    print(f"Database '{self.database}' created successfully.")
                
                # Connect to the specific database
                self.connection.database = self.database
                print(f"Connected to database '{self.database}'.")
        
        except Error as e:
            print(f"Error connecting to MySQL: {e}")

    def disconnect(self):
        if self.connection and self.connection.is_connected():
            if self.cursor:
                self.cursor.close()
            self.connection.close()
            print("MySQL connection closed.")

    def execute_query(self, query):
        try:
            self.cursor.execute(query)
            self.connection.commit()
            print("Query executed successfully.")
        except Error as e:
            print(f"Error executing query: {e}")

    # Add more methods for database operations as needed


==================================================

File: processData.py
==================================================
from operator import le
from os import read
import pandas as pd
from utilities import calculate_posted_time
from gpt import JobAnalyzer
import PyPDF2

# Class to process data

class ProcessData():
    
    def __init__(self, data):
        self.df_new = self.create_df(data)
        self.resume = self.read_pdf_resume("resume.pdf")
        print(len(self.df_new))
        self.remove_duplicates(self.df_new)
        print(len(self.df_new))
        self.add_posted_date(self.df_new)
        self.compare_df_csv()
        self.df_new.to_csv("job_application_pre_processing.csv", index=False)
        
    def compare_df_csv(self):
        old_df = pd.read_csv("job_application.csv")
        # Get the set of job_ids from old_df
        existing_job_ids = set(old_df['job_id'])

        #Remove rows from self.new_df where job_id is in existing_job_ids
        self.df_new = self.df_new[~self.df_new['job_id'].isin(existing_job_ids)]
    
    def create_df(self, data):
        return pd.DataFrame(data)
    
    def remove_duplicates(self, df):
        self.df_new = df.drop_duplicates(subset=['job_id'], keep='first')
        self.df_new = self.custom_drop_duplicates(df, 'apply_link')
    
    def custom_drop_duplicates(self, df, column):
    # Keep track of seen non-empty values
        seen = set()
    
        def keep(value):
            if value == "":
                return True
            if value not in seen:
                seen.add(value)
                return True
            return False
    
        return df[df[column].apply(keep)]
    
    def add_posted_date(self, df):
        df['posted_date'] = df['days_ago'].apply(lambda x: calculate_posted_time(x))
    
    async def analyze_job(self):
        try:
            analyzer = JobAnalyzer(self.df_new, self.resume)
            df_new, df_update = await analyzer.process_jobs()
            
            # Merge new columns based on job_id
            self.df_new = pd.merge(self.df_new, df_new, on='job_id', how='left')
            
            # Update existing columns
            self.df_new.update(df_update)
            
            self.append_data_csv()
        except Exception as e:
            print(f"An error occurred in pandas operations: {str(e)}")
            
    def append_data_csv(self):
        with open('job_application.csv', 'a') as f:
            f.write('\n')
        # Now append the new data
        self.df_new.to_csv('job_application.csv', mode='a', header=False, index=False)
       
    @staticmethod
    def read_pdf_resume(file_path):
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            for page in reader.pages:
                text += page.extract_text()
        return text

==================================================

File: utilities.py
==================================================
from urllib.parse import urlencode, quote_plus
from datetime import datetime, timedelta
import re
import pytz

def calculate_posted_time(time_ago_string):
    try:
        current_time = datetime.now()
        
        
        # Extract number and unit from the input string
        match = re.match(r'(\d+)\s+(\w+)\s+ago', time_ago_string)
        if not match:
            raise ValueError("Invalid input format")
        
        number, unit = int(match.group(1)), match.group(2).lower()
        
        # Define time units
        units = {
            'second': timedelta(seconds=1),
            'minute': timedelta(minutes=1),
            'hour': timedelta(hours=1),
            'day': timedelta(days=1),
            'week': timedelta(weeks=1),
            'month': timedelta(days=30),  # Approximation
            'year': timedelta(days=365)  # Approximation
        }
        
        # Handle plural forms
        if unit.endswith('s'):
            unit = unit[:-1]
        
        if unit not in units:
            raise ValueError("Invalid time unit")
        
        # Calculate the posted time
        posted_time = current_time - (units[unit] * number)
        
        return posted_time
    
    except Exception as e:
        print(f"An error occurred in days ago: {str(e)}")
        return datetime.now()

def convert_to_iso_time(date):
    # Your local time
    local_time = datetime.strptime(date, "%Y-%m-%d %H:%M:%S.%f")

    # Get your local timezone
    local_tz = pytz.timezone('America/New_York')  # Replace with your actual timezone, e.g., 'America/New_York'

    # Localize the datetime
    local_time_with_tz = local_tz.localize(local_time)

    # Convert to UTC
    utc_time = local_time_with_tz.astimezone(pytz.UTC)

    # Format in ISO 8601
    iso_time = utc_time.isoformat()

    print(iso_time)


def generate_linkedin_job_search_url(keyword, sort_by="DD", time_filter=2, experience_level="2,3", distance=25, industry=None):
    # industry: LinkedIn uses a hierarchical system for industries.
    # Examples include "4" (Computer Software), "6" (Internet),
    # "51" (Information Technology and Services)
    # Refer to LinkedIn's current industry taxonomy for a complete list

    # sortBy: Options include:
    # "R" (Most relevant), "DD" (Most recent), "PA" (Most viewed)

    # time_filter: Options include:
    # defualts to last 2 days, you can give how many days back you want to search

    # experience_level: Options include:
    # "1" (Internship), "2" (Entry level), "3" (Associate),
    # "4" (Mid-Senior level), "5" (Director), "6" (Executive)
    # Use comma-separated values for multiple levels, e.g., "2,3"

    # distance: Numeric value representing the search radius in miles or kilometers
    
    time_filter = "r" + str(time_filter*86400)  # Convert days to seconds
    
    base_url = "https://www.linkedin.com/jobs/search/?"
    
    params = {
        "keywords": keyword,
        "f_F": "it%2Canls",
        "f_I": industry,
        "sortBy": sort_by,
        "f_TPR": time_filter,
        "f_E": experience_level,
        "distance": distance,
        "geoId": "101174742",  # You may want to make this parameter customizable
        "origin": "JOB_SEARCH_PAGE_JOB_FILTER",
        "refresh": "false",
        "spellCorrectionEnabled": "true"
    }
    
    # Remove any None values from the params
    params = {k: v for k, v in params.items() if v is not None}
    
    # Encode the parameters
    encoded_params = urlencode(params, quote_via=quote_plus)
    
    # Combine the base URL with the encoded parameters
    full_url = base_url + encoded_params
    
    return full_url


==================================================

File: main.py
==================================================
# from g4f.client import Client
# import os.path
# from g4f.cookies import set_cookies_dir, read_cookie_files
# from g4f.Provider import (Liaobots)
# import g4f

# import g4f.debug

# g4f.debug.logging = True
# cookies_dir = os.path.join(os.path.dirname(__file__), "har_and_cookies")
# set_cookies_dir(cookies_dir)
# read_cookie_files(cookies_dir)


# client = Client(Liaobots)
# response = client.chat.completions.create(
#     model=g4f.models.gpt_4o,
#     messages=[{"role": "user", "content": "Tell me about Coveo company for my interview"}],
# )              
# print(response.choices[0].message.content)

# from LinkedIn.linkedIn import LinkedIn

# linkedin = LinkedIn("krishnavalliappan02@gmail.com", "YE$35A!GJjn@AQ!3")
# linkedin.search_jobs_runner("data analyst", time_filter=2)
# from datamanger import DataManager

# dm = DataManager("localhost", "root", "welcome123", "linkedin_data")

# # Connect to the database (creates it if it doesn't exist)
# dm.connect()

# # Execute a sample query (e.g., create a table)
# # dm.execute_query("""
# #     CREATE TABLE IF NOT EXISTS users (
# #         id INT AUTO_INCREMENT PRIMARY KEY,
# #         name VARCHAR(255),
# #         email VARCHAR(255)
# #     )
# # """)

# # Disconnect when done
# dm.disconnect()

# In main.py:
from processData import ProcessData
import asyncio
from linkedin.linkedIn import LinkedIn
from ResumeManager.resumeManager import ResumeManager
import os
import pandas as pd
from dotenv import load_dotenv
from notion_manager import NotionManager

load_dotenv()

async def main():
    linkedin_email = os.environ.get('LINKEDIN_EMAIL')
    linkedin_password = os.environ.get('LINKEDIN_PASSWORD')
    database_id = "7585377689d14a70bce0e38935403a1b"
    
    if not linkedin_email or not linkedin_password:
        raise ValueError("LinkedIn credentials not set in environment variables")

    try:
        linkedin = LinkedIn(linkedin_email, linkedin_password)
        linkedin.search_jobs_runner("Data Analyst", time_filter=1)
        data = linkedin.scraped_job_data
        # data = pd.read_csv("job_application_pre_processing.csv")
        process_data = ProcessData(data)
        await process_data.analyze_job()
        # create resumes and cover_letters
        new_df = process_data.df_new
        ResumeManager(new_df)
        notion = NotionManager(database_id=database_id)
        notion.one_way_sync(new_df)
    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    asyncio.run(main())

==================================================

File: gpt.py
==================================================
import json
import asyncio
from enum import Enum
from typing import List, Optional, Dict, Tuple, Any
from weakref import proxy
from pydantic import BaseModel, Field
from langchain.llms.base import LLM
import g4f
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from g4f.client import Client
from g4f.cookies import set_cookies_dir, read_cookie_files
import os
import re
import PyPDF2
import pandas as pd
from Utilities.proxies import ProxyRotator
from g4f.Provider import You, Liaobots, ChatgptAi, Bing, RetryProvider

proxy_rotator = ProxyRotator()

g4f.debug.logging = True
cookies_dir = os.path.join(os.path.dirname(__file__), "har_and_cookies")
set_cookies_dir(cookies_dir)
read_cookie_files(cookies_dir)


class EducationalLLM(LLM):
    @property
    def _llm_type(self) -> str:
        return "custom"

    def _call(self, prompt: str, stop: Optional[List[str]] = None, run_manager=None, **kwargs) -> str:
        max_retries = 2
        for attempt in range(max_retries):
            try:
                client = Client(proxies=proxy_rotator.get_proxy())
                response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": prompt}],
                )
                out = response.choices[0].message.content
                if stop:
                    stop_indexes = (out.find(s) for s in stop if s in out)
                    min_stop = min(stop_indexes, default=-1)
                    if min_stop > -1:
                        out = out[:min_stop]
                print("with proxy success")
                return out
            except Exception as e:
                print(f"Attempt {attempt + 1} failed with proxy: {str(e)}")
                proxy_rotator.remove_current_proxy()
                if not proxy_rotator.proxies:
                    proxy_rotator.refresh_proxies()                  
        # If all proxy attempts fail, try without a proxy
        try:
            print("Attempting to connect without a proxy...")
            client = Client()
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
            )
            out = response.choices[0].message.content
            if stop:
                stop_indexes = (out.find(s) for s in stop if s in out)
                min_stop = min(stop_indexes, default=-1)
                if min_stop > -1:
                    out = out[:min_stop]
            print("without proxy success")
            return out
        except Exception as e:
            print("Attempting to connect without a proxy...")
            client = Client()
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
            )
            out = response.choices[0].message.content
            if stop:
                stop_indexes = (out.find(s) for s in stop if s in out)
                min_stop = min(stop_indexes, default=-1)
                if min_stop > -1:
                    out = out[:min_stop]
            print("without proxy success using gpt-3.5-turbo")
            return out
            # raise Exception(f"Failed to get a response after multiple attempts with proxies and without proxy: {str(e)}")


class JobCategory(str, Enum):
    DATA = "data role"
    BUSINESS = "business role"
    IT = "IT role"

class JobAnalysisOutput(BaseModel):
    skills_in_priority_order: List[str] = Field(description="Top 3 technical tool and tech stack mentioned in job description which is I know as per my resume")
    job_category: JobCategory = Field(description="Categorization of the job role")
    why_this_company: str = Field(description="Personalized 'Why This Company' paragraph")
    why_me: str = Field(description="Personalized 'Why Me' paragraph")
    job_position_title: str = Field(description="Formatted job position title in English")
    company_name: str = Field(description="Formatted company name in English")

def preprocess_job_analysis(result: Tuple[str, Optional[JobAnalysisOutput]]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    job_id, result = result
    new_columns = {
        "job_id": job_id,
        "top_skills": None,
        "job_category": None,
        "why_this_company": None,
        "why_me": None,
    }
    update_columns = {
        "job_id": job_id,
        "job_position_title": None,
        "company_name": None
    }
    
    if result is None:
        return new_columns, update_columns
    
    try:
        skills = result.skills_in_priority_order[:3]
        if "Python" not in skills and "Python" in result.skills_in_priority_order:
            skills = skills[:2] + ["Python"]
        skills_str = ", ".join(skills[:-1]) + ", and " + skills[-1] if len(skills) > 1 else skills[0]
        
        new_columns = {
            "top_skills": skills_str,
            "job_category": result.job_category.value,
            "why_this_company": result.why_this_company,
            "why_me": result.why_me,
        }
        
        update_columns = {
            "job_position_title": result.job_position_title,
            "company_name": result.company_name
        }
        return new_columns, update_columns
    
    except AttributeError as e:
        print(f"AttributeError in preprocess_job_analysis: {e}")
        return new_columns, update_columns

class JobAnalyzer:
    def __init__(self, df: Optional[pd.DataFrame] = None, resume_text: Optional[str] = None):
        self.llm = EducationalLLM()
        self.df = df
        self.resume_text = resume_text
        proxy_rotator.get_proxy()

    def _get_prompt(self) -> PromptTemplate:
        template = """
        Analyze the following job description and resume, then provide the requested information:

        Job Description:
        {job_description}

        Resume:
        {resume}

        Company Name: {company_name}
        
        Job Position Title: {job_position_title}
        

        Please provide the following information:
        1. List the top 3 technical tools and the tech stack that are mentioned in the job description, which I'm familiar with as per my given resume. Include Python by default, listed in priority order.
        2. Categorization of the job role: data role, business role, or IT role
        3. A personalized 'Why This Company' paragraph (see instructions below)
        4. A personalized 'Why Me' paragraph (see instructions below)
        5. A formatted job position title in English, remove any unwanted characters which can't be allowed in directory creation and ensuring it's professional which I can use it in my resume. Make it short if its too long and a typical one.
        6. A formatted company name in English, removing any unwanted characters which can't be allowed in directory or file creation. If the company name is only in French, leave it as is.

        Instructions for 'Why This Company':
        Generate a paragraph that includes the following elements: Do web search and know about company.
        • An understanding of the company's mission, vision, and values.
        • Specific details about the company's products, services, and market position.
        • A mention of the company's reputation and culture.
        • How the company's direction and growth opportunities align with the candidate's career aspirations.
        • Why the candidate is excited about the company.
        example: 'Affirm’s innovative approach to consumer finance is a major factor that draws me to this role. Affirm’s
commitment to transparency and creating consumer-friendly financial products aligns perfectly with my
values and career goals. I am particularly impressed by Affirm’s dedication to eliminating hidden fees and
providing clear, upfront information to consumers, which resonates with my passion for ethical financial
practices. The opportunity to work at a company that leverages cutting-edge technology to optimize
portfolio economics and consumer growth is incredibly exciting to me. I am eager to contribute to Affirm’s
mission of delivering honest financial products that improve lives.'
        length: follow the length of the example provided.

        Instructions for 'Why Me':
        Generate a paragraph that includes the following elements:
        • Relevant experience and skills that match the job requirements.
        • Specific achievements that demonstrate the candidate's capabilities.
        • How the candidate's skills and experience align with the company's needs.
        • The candidate's passion for the industry or role.
        • A brief, professional mention of hoping to master pizza-making before the interview call.
        example: 'With over three years of experience as a Data Analyst, I bring strong analytical skills and proficiency in
SQL, Python, and VBA, essential for the Quantitative Analyst role at Affirm. My achievements include
boosting sales projections by 15% with predictive models and enhancing system reliability through
automated monitoring. My collaborative and problem-solving abilities make me a great fit for this role. I
am confident my technical expertise and passion for fintech innovation will significantly contribute to
Affirm’s success. I look forward to discussing how I can add value, hopefully before perfecting my
homemade pizza recipe!'
        length: follow the length of the example provided.

        Format your response as a JSON object with the following keys:
        skills_in_priority_order, job_category, why_this_company, why_me, job_position_title, company_name
        """
        return PromptTemplate(
            input_variables=["job_description", "resume", "company_name", "job_position_title"],
            template=template
        )
        
    def _extract_json(self, text: str) -> Dict[str, Any]:
        """Extract JSON content from the text and return as a dictionary."""
        match = re.search(r'\{[\s\S]*\}', text)
        if match:
            try:
                return json.loads(match.group(0))
            except json.JSONDecodeError:
                print(f"Failed to parse JSON: {match.group(0)}")
                return {}
        else:
            print(f"No JSON found in the text: {text}")
            return {}

    async def analyze_job(self, job_description, resume, company_name, job_position_title, job_id, attempts=0):
        if attempts >= 3:
            print(f"Failed to analyze job after 3 attempts for {job_position_title} at {company_name}")
            return None

        prompt = self._get_prompt()
        chain = (
            {"job_description": RunnablePassthrough(), "resume": RunnablePassthrough(), "company_name": RunnablePassthrough(), "job_position_title": RunnablePassthrough()}
            | prompt
            | self.llm
            | self._extract_json
        )
        result = await chain.ainvoke({"job_description": job_description, "resume": resume, "company_name": company_name, "job_position_title": job_position_title})
        
        try:
            analysis_output = JobAnalysisOutput(**result)
            return job_id, analysis_output
        except ValueError as e:
            print(f"Validation error (attempt {attempts + 1}): {e}")
            print(f"Raw result: {result}")
            # Recursive call with incremented attempts
            return await self.analyze_job(job_description, resume, company_name, job_position_title, job_id, attempts + 1)


    async def process_jobs(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        if self.df is None or self.resume_text is None:
            raise ValueError("DataFrame and resume text must be provided.")

        tasks = [self.analyze_job(row['job_description'], self.resume_text, row['company_name'], row["job_position_title"], row["job_id"])
            for _, row in self.df.iterrows()]

        results = []
        completed_tasks = 0
        total_tasks = len(tasks)
        print(total_tasks)

        for i in range(0, total_tasks, min(total_tasks, 5)):
            batch = tasks[i:i+min(total_tasks, 5)]
            batch_results = await asyncio.gather(*batch)
            results.extend(batch_results)
            completed_tasks += len(batch)
            print(f"-----------\n Processed {completed_tasks} out of {total_tasks} jobs \n -----------")

        print(f"All tasks completed. Total jobs processed: {completed_tasks}")

        valid_results = [result for result in results if result is not None]

        if not valid_results:
            print("No valid results were obtained.")
            return pd.DataFrame(), pd.DataFrame()

        new_columns, update_columns = zip(*[preprocess_job_analysis(result) for result in valid_results])
        
        df_new = pd.DataFrame(new_columns)
        df_update = pd.DataFrame(update_columns)
        
        # Add job_id to both DataFrames
        df_new['job_id'] = [result[0] for result in valid_results]
        df_update['job_id'] = [result[0] for result in valid_results]
        
        return df_new, df_update



# async def main():
#     def read_pdf_resume(file_path):
#         with open(file_path, 'rb') as file:
#             reader = PyPDF2.PdfReader(file)
#             text = ""
#             for page in reader.pages:
#                 text += page.extract_text()
#         return text
    
#     df = pd.read_csv("job_application_pre_processing.csv")
#     df = df.drop_duplicates(subset='job_id', keep='first')
    
#     # analyzer = JobAnalyzer(df, read_pdf_resume("resume.pdf")) 
#     # df_new, df_update = await analyzer.process_jobs()
    
#     # df_new.to_csv("new_columns.csv", index=False)
#     # df_update.to_csv("update_columns.csv", index=False)
    
#     df_new = pd.read_csv("new_columns.csv")
#     df_update = pd.read_csv("update_columns.csv")
    
#     # Merge new columns
#     df = pd.merge(df, df_new, on='job_id', how='left')
#     # Update existing columns
#     df.update(df_update)
    
#     df.to_csv("test_csv.csv", index=False)

# if __name__ == "__main__":
#     asyncio.run(main())

==================================================

File: Utilities/proxies.py
==================================================
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import random

class ProxyRotator:
    def __init__(self):
        self.proxies = None
        self.current_proxy = None

    def get_proxy(self):
        if self.proxies:
            self.current_proxy = random.choice(self.proxies)
            return {'all': self.current_proxy, 'https': self.current_proxy, 'http': self.current_proxy}
        else:
            self.proxies = self.get_working_proxies()
            self.current_proxy = random.choice(self.proxies)
            return {'all': self.current_proxy, 'https': self.current_proxy}

    def remove_current_proxy(self):
        if self.current_proxy in self.proxies:
            self.proxies.remove(self.current_proxy)

    @staticmethod
    def get_proxies():
        url = 'https://free-proxy-list.net/'
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        proxies = []
        table = soup.find('table', class_='table table-striped table-bordered')
        if table:
            tbody = table.find('tbody')
            if tbody:
                rows = tbody.find_all('tr')
                for row in rows:
                    tds = row.find_all('td')
                    ip = tds[0].text.strip()
                    port = tds[1].text.strip()
                    proxies.append(f'http://{ip}:{port}')
            else:
                print("No table body found")
        else:
            print("No table found")
        return proxies

    @staticmethod
    def check_proxy(proxy):
        try:
            response = requests.get('https://httpbin.org/ip', proxies={'http': proxy, 'https': proxy}, timeout=5)
            if response.status_code == 200:
                return proxy
        except:
            pass
        return None

    def get_working_proxies(self):
        proxies = self.get_proxies()
        working_proxies = []
        
        with ThreadPoolExecutor(max_workers=20) as executor:
            future_to_proxy = {executor.submit(self.check_proxy, proxy): proxy for proxy in proxies}
            for future in as_completed(future_to_proxy):
                result = future.result()
                if result:
                    working_proxies.append(result)
                    
        print(f"proxy count: {len(working_proxies)}")
        
        return working_proxies

    def refresh_proxies(self):
        self.proxies = self.get_working_proxies()


# if __name__ == "__main__":
#     proxy_rotator = ProxyRotator()
    
#     print("Initial working proxies:")
#     for proxy in proxy_rotator.proxies:
#         print(proxy)
    
#     print("\nGetting a proxy:")
#     proxy = proxy_rotator.get_proxy()
#     print(proxy)
    
#     print("\nRemoving current proxy:")
#     proxy_rotator.remove_current_proxy()
#     print(f"Proxies left: {len(proxy_rotator.proxies)}")
    
#     print("\nRefreshing proxies:")
#     proxy_rotator.refresh_proxies()
#     print(f"New proxy count: {len(proxy_rotator.proxies)}")


==================================================

File: ResumeManager/test.py
==================================================


==================================================

File: ResumeManager/resumeManager.py
==================================================
import re
from docx import Document
from datetime import datetime
import pandas as pd
import os
import sys
import subprocess
import re

# current_dir = os.path.dirname(os.path.abspath(__file__))
# sys.path.insert(0, current_dir)


class ResumeManager():
    def __init__(self, df) -> None:
        self.df = df
        for index, row in self.df.iterrows():
            self.edit_resume_cover(row)
    
    def find_resume_cover_template(self, row):
        resume_path = f"ResumeManager/ResumeTemplates/resume_data role.docx"
        cover_path = f"ResumeManager/ResumeTemplates/cover_data role.docx"
        try:
            job_category = row["job_category"]
            if job_category:
                resume_path = f"ResumeManager/ResumeTemplates/resume_{job_category}.docx"
                cover_path = f"ResumeManager/ResumeTemplates/cover_{job_category}.docx"
            return resume_path, cover_path
        except Exception as e:
            print(f"An error occurred: {str(e)}")
            return resume_path, cover_path
    
    def table_edit_replace(self, doc, target, value):
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    if target in cell.text:
                        for paragraph in cell.paragraphs:
                            for run in paragraph.runs:
                                run.text = run.text.replace(target, value)
    
    def paragraph_edit_replace(self, doc, target, value):
        for paragraph in doc.paragraphs:
            if target in paragraph.text:
                inline = paragraph.runs
                # Loop added to work with runs (strings with same style)
                for i in range(len(inline)):
                    if target in inline[i].text:
                        text = inline[i].text.replace(target, value)
                        inline[i].text = text

    
    def edit_resume_cover(self, row):
        # find resume and cover letter template path based on job category
        resume_path, cover_path = self.find_resume_cover_template(row)
        
        resume_doc = Document(resume_path)
        cover_doc = Document(cover_path)
        top_skills = row["top_skills"]
        company = row["company_name"]
        job_role = row["job_position_title"]
        company_location = row["location"]
        today_date = datetime.now().strftime("%d-%b-%Y")
        why_company = row["why_this_company"]
        why_me = row["why_me"]
        
        if not job_role:
            job_role = "Data Analyst"
        if not top_skills:
            top_skills = "Python, SQL, Power BI, Excel, Machine Learning"
            

        # output path
        output_path = f"ResumeManager/OutputResumes/{company}_{job_role}_{today_date}/"
        # Create the directory if it doesn't exist
        os.makedirs(output_path, exist_ok=True)
        
        cover_input_dic = {
            "[job role]": job_role,
            "[company name]": company,
            "[company location]": company_location,
            "[job role]": job_role,
            "[date]": today_date,
            "[why company]": why_company,
            "[why me]": why_me
        }
        
        self.create_resume(resume_doc, job_role, top_skills, output_path)
        self.create_cover(cover_doc,output_path, cover_input_dic)
        
        
    def create_cover(self, cover_doc, output_path, cover_input_dic):
        self.table_edit_replace(cover_doc, "[job role]", cover_input_dic["[job role]"])
        for key, value in cover_input_dic.items():
            self.paragraph_edit_replace(cover_doc, key, value)
        docx_path = os.path.join(output_path, "Krishnakumar Cover Letter.docx")
        cover_doc.save(docx_path)
        self.save_to_pdf(output_path, docx_path)
        
    
    def create_resume(self, resume_doc, job_role, top_skills, output_path):
        self.table_edit_replace(resume_doc, "[job role]", job_role)
        self.paragraph_edit_replace(resume_doc, "[top skills]", top_skills)
        docx_path = os.path.join(output_path, "Krishnakumar Resume.docx")
        resume_doc.save(docx_path)
        self.save_to_pdf(output_path, docx_path)
        
        
    def save_to_pdf(self, output_path, docx_path):
        # Convert .docx to .pdf using LibreOffice
        libreoffice_path = "/Applications/LibreOffice.app/Contents/MacOS/soffice"
        # resume_pdf_path = os.path.join(output_path, f"Krishnakumar {doc_type}.pdf")
        try:
            subprocess.run([
                libreoffice_path,
                "--headless",
                "--convert-to",
                "pdf",
                "--outdir",
                output_path,
                docx_path
            ], check=True)
        except subprocess.CalledProcessError as e:
            print(f"Error converting to PDF: {e}")

==================================================

File: linkedin/linkedInscrapper.py
==================================================
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium_stealth import stealth
import pickle
import os
import sys
import logging
import re
import random
from utilities import generate_linkedin_job_search_url

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

# Set up logging
logging.basicConfig(filename='linkedin_scraper.log', level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

def create_stealth_driver():
    options = Options()
    options.add_argument("start-maximized")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    driver = webdriver.Chrome(options=options)
    
    stealth(driver,
        languages=["en-US", "en"],
        vendor="Google Inc.",
        platform="Win32",
        webgl_vendor="Intel Inc.",
        renderer="Intel Iris OpenGL Engine",
        fix_hairline=True,
    )
    
    return driver

class LinkedInScraper:
    def __init__(self, username, password, cookie_file="linkedin_cookies.pkl"):
        self.username = username
        self.password = password
        self.cookie_file = cookie_file
        self.driver = create_stealth_driver()
        self.initial_start()
    
    def initial_start(self):
        try:
            if os.path.exists(self.cookie_file):
                self.driver.get("https://www.linkedin.com")
                self.load_cookies()
                self.driver.refresh()
                
                time.sleep(5)
                
                if "feed" not in self.driver.current_url:
                    logging.info("Cookies expired, logging in again")
                    self.login_to_linkedin()
                else:
                    logging.info("Successfully logged in using cookies")
            else:
                self.login_to_linkedin()
            
            self.save_cookies()
            
        except Exception as e:
            logging.error(f"An error occurred during initial start: {str(e)}")
    
    def login_to_linkedin(self):
        try:
            self.driver.get("https://www.linkedin.com/login")
            time.sleep(random.uniform(2, 8))
            
            username_field = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "username"))
            )
            username_field.send_keys(self.username)
            
            time.sleep(random.uniform(1, 4))
            
            password_field = self.driver.find_element(By.ID, "password")
            password_field.send_keys(self.password)
            
            time.sleep(random.uniform(1, 2))
            
            login_button = self.driver.find_element(By.XPATH, "//button[@type='submit']")
            login_button.click()
            
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "global-nav"))
            )
            
            logging.info("Successfully logged in to LinkedIn")
        except Exception as e:
            logging.error(f"Error during login: {str(e)}")
            raise
        
    def save_cookies(self):
        try:
            cookies = self.driver.get_cookies()
            with open(self.cookie_file, "wb") as f:
                pickle.dump(cookies, f)
            logging.info(f"Saved cookies to {self.cookie_file}")
        except Exception as e:
            logging.error(f"Error saving cookies: {str(e)}")
    
    def load_cookies(self):
        try:
            if os.path.exists(self.cookie_file):
                with open(self.cookie_file, "rb") as f:
                    cookies = pickle.load(f)
                    for cookie in cookies:
                        self.driver.add_cookie(cookie)
                logging.info(f"Loaded cookies from {self.cookie_file}")
            else:
                logging.warning(f"Cookie file {self.cookie_file} not found")
        except Exception as e:
            logging.error(f"Error loading cookies: {str(e)}")
    
    def scroll_to_bottom_page(self):
        try:
            SCROLL_PAUSE_TIME = 2
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            while True:
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(SCROLL_PAUSE_TIME)
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height
        except Exception as e:
            logging.error(f"Error during scrolling: {str(e)}")
    
    def scroll_to_bottom_element(self, by, element_value, scroll_full=True):
        try:
            element = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((by, element_value))
            )
            
            last_height = self.driver.execute_script("return arguments[0].scrollHeight;", element)
            
            while True:
                if scroll_full:
                    self.driver.execute_script("arguments[0].scrollTo(0, arguments[0].scrollHeight);", element)
                else:
                    visible_height = self.driver.execute_script("return arguments[0].clientHeight;", element)
                    self.driver.execute_script(f"arguments[0].scrollBy(0, {visible_height * 0.32});", element)
                
                time.sleep(2)
                
                new_height = self.driver.execute_script("return arguments[0].scrollHeight;", element)
                
                if new_height == last_height:
                    break
                
                last_height = new_height
                
                if not scroll_full:
                    break

            ActionChains(self.driver).move_to_element(element).perform()
        except Exception as e:
            logging.error(f"Error during element scrolling: {str(e)}")
    
    def search_job(self, keyword, **kwargs):
        try:
            url = generate_linkedin_job_search_url(keyword, **kwargs)
            self.driver.get(url)
            logging.info(f"Searching for jobs with keyword: {keyword}")
            result_title = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "results-list__title"))
            ).text
            no_of_results = int(self.driver.find_element(By.CSS_SELECTOR, "div.jobs-search-results-list__subtitle span").text.split()[0])    
            logging.info(f"Search results loaded successfully for {result_title} with {no_of_results} results")
            
            return result_title, no_of_results
        except Exception as e:
            logging.error(f"Error during job search: {str(e)}")
            return None

    def page_clicker(self, page_no):
        try:
            button = WebDriverWait(self.driver, 10).until(
                EC.element_to_be_clickable((By.XPATH, f"//button[@aria-label='Page {page_no}']"))
            )
            button.click()
            time.sleep(5)
            logging.info(f"Successfully clicked the 'Page {page_no}' button")
        except Exception as e:
            logging.error(f"Error clicking page button: {str(e)}")
            
    def get_job_id(self, href):
        try:
            return href.split("/")[5]
        except Exception as e:
            logging.error(f"Error extracting job ID: {str(e)}")
            return None
    
    def remove_characters(self, text):
        try:
            number = re.findall(r'\d+', text)
            return int(number[0]) if number else 0
        except Exception as e:
            logging.error(f"Error removing characters: {str(e)}")
            return 0
    
    def extract_job_details(self, job_element):
        salary = workplace = job_type = experience_level = None
        try:
            salary_element = job_element.find_element(By.CSS_SELECTOR, "span > span:not([class])")
            children = salary_element.find_elements(By.XPATH, "./*")
            salary = salary_element.text.strip() if len(children) == 0 else None
        except Exception as e:
            logging.error(f"Error extracting salary: {str(e)}")

        try:
            span_models = job_element.find_elements(By.XPATH, 
                ".//span[contains(@class, 'ui-label ui-label--accent-3 text-body-small')] | " +
                ".//span[contains(@class, 'job-details-jobs-unified-top-card__job-insight-view-model-secondary')]")
            
            for element in span_models:
                text = element.text.strip() if len(element.find_elements(By.XPATH, "./*")) == 0 else element.find_element(By.CSS_SELECTOR, "span[aria-hidden='true']").text.strip()
                
                if text in ['Full-time', 'Part-time', 'Contract', 'Temporary', 'Internship', "Other"]:
                    job_type = text
                elif text in ['Entry level', 'Associate', 'Mid-Senior level', 'Director', 'Executive']:
                    experience_level = text
                elif text in ['Remote', 'Hybrid', "On-site"]:
                    workplace = text
                
        except Exception as e:
            logging.error(f"Error extracting job details: {str(e)}")

        return salary, workplace, job_type, experience_level
        
    def apply_link_finder(self, element):
        is_easy_apply = False
        apply_link = None
        try:
            button_element = element.find_element(By.CSS_SELECTOR, "div.jobs-apply-button--top-card button")
            if button_element.find_element(By.TAG_NAME, "span").text.strip() == "Easy Apply":
                is_easy_apply = True
            else:
                button_element.click()
                time.sleep(2)
                
                if len(self.driver.window_handles) > 1:
                    self.driver.switch_to.window(self.driver.window_handles[-1])
                    apply_link = self.driver.current_url
                    self.driver.close()
                    time.sleep(4)
                    self.driver.switch_to.window(self.driver.window_handles[0])
        except Exception as e:
            logging.error(f"Error finding apply link: {str(e)}")
        
        return is_easy_apply, apply_link
                
    def extract_industry(self, element):
        try:
            industry_row_span = element.find_elements(By.CSS_SELECTOR, "li.job-details-jobs-unified-top-card__job-insight")[1].find_element(By.CSS_SELECTOR, "span").text
            if "·" in industry_row_span:
                 return industry_row_span.split("·")[1].strip()
            elif "employees" not in industry_row_span:
                return industry_row_span.strip()
        except Exception as e:
            logging.error(f"Error extracting industry: {str(e)}")
        return None
        
    def crab_job_details(self):
        job_data = {
            'job_position_title': None,
            'job_id': None,
            'job_position_link': None,
            'company_logo': None,
            'company_name': None,
            'location': None,
            'days_ago': None,
            'no_of_applicants': None,
            'salary': None,
            'workplace': None,
            'job_type': None,
            'experience_level': None,
            'industry': None,
            'is_easy_apply': False,
            'apply_link': None,
            'job_description': None
        }

        try:
            time.sleep(random.uniform(2, 8))
            job_details = self.driver.find_element(By.CSS_SELECTOR, "div.jobs-search__job-details--wrapper")

            # Job position, job id, job link
            try:
                job_position_element = job_details.find_element(By.CSS_SELECTOR, "h1[class*='t-24 t-bold'] a")
                job_data['job_position_title'] = job_position_element.text
                job_data['job_position_link'] = job_position_element.get_attribute("href")
                job_data['job_id'] = self.get_job_id(job_data['job_position_link'])
            except Exception as e:
                logging.error(f"Error extracting job position details: {str(e)}")

            # Company name, image
            try:
                job_data['company_logo'] = job_details.find_element(By.CSS_SELECTOR, "div.flex-1 a.app-aware-link img").get_attribute('src')
                job_data['company_name'] = job_details.find_element(By.CSS_SELECTOR, "div.job-details-jobs-unified-top-card__company-name a.app-aware-link").text
            except Exception as e:
                logging.error(f"Error extracting company details: {str(e)}")

            # Location, posted ago, no_of_applicants
            try:
                primary_description_element = job_details.find_elements(By.CSS_SELECTOR, "div.job-details-jobs-unified-top-card__primary-description-container div span.tvm__text")
                job_data['location'] = primary_description_element[0].text if len(primary_description_element) > 0 else None
                job_data['days_ago'] = primary_description_element[2].find_element(By.CSS_SELECTOR, "span:not([class])").text if len(primary_description_element) > 2 else None
                job_data['no_of_applicants'] = self.remove_characters(primary_description_element[4].text) if len(primary_description_element) > 4 else None
            except Exception as e:
                logging.error(f"Error extracting job metadata: {str(e)}")

            # Job details (salary, workplace, job_type, experience_level)
            try:
                highlight_element = job_details.find_element(By.CSS_SELECTOR, "li.job-details-jobs-unified-top-card__job-insight--highlight")
                job_data['salary'], job_data['workplace'], job_data['job_type'], job_data['experience_level'] = self.extract_job_details(highlight_element)
            except Exception as e:
                logging.error(f"Error extracting job details: {str(e)}")

            # Industry
            try:
                job_data['industry'] = self.extract_industry(job_details)
            except Exception as e:
                logging.error(f"Error extracting industry: {str(e)}")

            # Apply link and easy apply status
            try:
                job_data['is_easy_apply'], job_data['apply_link'] = self.apply_link_finder(job_details)
            except Exception as e:
                logging.error(f"Error extracting apply link: {str(e)}")

            # Job description
            try:
                job_data['job_description'] = job_details.find_element(By.CSS_SELECTOR, "article.jobs-description__container").text.replace('\n', ' ')
            except Exception as e:
                logging.error(f"Error extracting job description: {str(e)}")

        except Exception as e:
            logging.error(f"Major error in crab_job_details: {str(e)}")

        return job_data


==================================================

File: linkedin/linkedIn.py
==================================================
import math
import logging
from selenium.webdriver.common.by import By
import os
import sys
import time

# Add the parent directory to the Python path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from linkedInscrapper import LinkedInScraper


# Set up logging
logging.basicConfig(filename='linkedin_search.log', level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

class LinkedIn:
    def __init__(self, username, password):
        self.username = username
        self.password = password
        self.linkedin = LinkedInScraper(self.username, self.password)
        self.scraped_job_data = []

    def search_jobs_runner(self, keyword, sort_by="DD", time_filter="r604800", experience_level="2,3", distance=25, industry=None):
        try:
            result_title, no_of_results = self.linkedin.search_job(keyword, sort_by=sort_by, time_filter=time_filter, 
                                                                   experience_level=experience_level, distance=distance, industry=industry)
            
            if result_title is None or no_of_results is None:
                logging.error("Failed to retrieve search results")
                return

            logging.info(f"Search results: {result_title}, Total jobs: {no_of_results}")

            total_pages = math.ceil(no_of_results / 25)
            for page in range(total_pages):
                try:
                    self._process_page(page)
                except Exception as e:
                    logging.error(f"Error processing page {page + 1}: {str(e)}")

                if page != total_pages - 1:
                    try:
                        self.linkedin.page_clicker(page + 2)
                    except Exception as e:
                        logging.error(f"Error clicking to next page: {str(e)}")
                        break

        except Exception as e:
            logging.error(f"An error occurred in search_jobs_runner: {str(e)}")

        finally:
            if len(self.scraped_job_data) > 0:
                logging.info(f"Successfully scraped {len(self.scraped_job_data)} job listings")
            self.linkedin.driver.quit()
            

    def _process_page(self, page):
        try:
            self.linkedin.scroll_to_bottom_element(By.CSS_SELECTOR, "div.jobs-search-results-list")
            ul_element = self.linkedin.driver.find_element(By.CSS_SELECTOR, "ul.scaffold-layout__list-container")
            li_elements = ul_element.find_elements(By.CSS_SELECTOR, "li.jobs-search-results__list-item")
            
            logging.info(f"Found {len(li_elements)} job listings on page {page + 1}")

            for i, li in enumerate(li_elements):
                try:
                    job_data = self._process_job_listing(li, i)
                    self.scraped_job_data.append(job_data)
                except Exception as e:
                    logging.error(f"Error processing job listing {i + 1} on page {page + 1}: {str(e)}")
                
                try:
                    self.linkedin.scroll_to_bottom_element(By.CSS_SELECTOR, "div.jobs-search-results-list", scroll_full=False)
                except Exception as e:
                    logging.warning(f"Error scrolling after processing job {i + 1}: {str(e)}") 

        except Exception as e:
            print("error while processing page")
            logging.error(f"Error processing page {page + 1}: {str(e)}")

    def _process_job_listing(self, li, index):
        try:
            div_clickable = li.find_element(By.CSS_SELECTOR, "div.job-card-container--clickable")
            div_clickable.click()
        except Exception as e:
            logging.warning(f"Couldn't click div.job-card-container--clickable for job {index + 1}, trying to click li: {str(e)}")
            li.click()

        try:
            job_data = self.linkedin.crab_job_details()
            logging.info(f"Successfully scraped job {index + 1}: {job_data.get('job_position_title', 'Unknown Title')}")
            
            return job_data
            
        except Exception as e:
            logging.error(f"Error scraping details for job {index + 1}: {str(e)}")

==================================================

